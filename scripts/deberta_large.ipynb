{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torchmetrics import ConfusionMatrix, Accuracy, Precision, Recall, F1\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "import data_prep "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, binary_df = data_prep.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 's-nlp/deberta-large-formality-ranker'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2formality = {0: \"formal\", 1: \"informal\"} # from model documentation on Hugging Face\n",
    "\n",
    "batch_size = 4\n",
    "predicted_labels = [] # 0 for informal, 1 for formal (consistent with dataset labels)\n",
    "\n",
    "for i in tqdm(range(0, len(binary_df), batch_size)):\n",
    "    texts = binary_df['sentence'][i:i + batch_size].tolist()\n",
    "\n",
    "    # prepare the input\n",
    "    encoding = tokenizer(\n",
    "        texts,\n",
    "        add_special_tokens=True,\n",
    "        return_token_type_ids=True,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "\n",
    "    # inference\n",
    "    output = model(**encoding)\n",
    "\n",
    "    batch_predicted_labels = []\n",
    "    for text_scores in output.logits.softmax(dim=1):\n",
    "        score_dict = {id2formality[idx]: score for idx, score in enumerate(text_scores.tolist())}\n",
    "        batch_predicted_labels.append(1 if score_dict['formal'] > score_dict['informal'] else 0)\n",
    "\n",
    "    predicted_labels.extend(batch_predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize metrics\n",
    "conf_matrix_metric = ConfusionMatrix(num_classes=2)\n",
    "accuracy_metric = Accuracy()\n",
    "precision_metric = Precision()\n",
    "recall_metric = Recall()\n",
    "f1_metric = F1()\n",
    "\n",
    "# Compute metrics\n",
    "conf_matrix = conf_matrix_metric(torch.tensor(predicted_labels), torch.tensor(binary_df['formal'].values))\n",
    "accuracy = accuracy_metric(torch.tensor(predicted_labels), torch.tensor(binary_df['formal'].values))\n",
    "precision = precision_metric(torch.tensor(predicted_labels), torch.tensor(binary_df['formal'].values))\n",
    "recall = recall_metric(torch.tensor(predicted_labels), torch.tensor(binary_df['formal'].values))\n",
    "f1 = f1_metric(torch.tensor(predicted_labels), torch.tensor(binary_df['formal'].values))\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix.numpy())\n",
    "print(\"Accuracy:\", accuracy.item())\n",
    "print(\"Precision:\", precision.item())\n",
    "print(\"Recall:\", recall.item())\n",
    "print(\"F1 Score:\", f1.item())\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
