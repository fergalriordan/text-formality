{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = {'train': 'train.csv', 'test': 'test.csv'}\n",
    "train_df = pd.read_csv(\"hf://datasets/osyvokon/pavlick-formality-scores/\" + splits[\"train\"])\n",
    "test_df = pd.read_csv(\"hf://datasets/osyvokon/pavlick-formality-scores/\" + splits[\"test\"])\n",
    "df = pd.concat([train_df, test_df])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ferga\\AppData\\Roaming\\Python\\Python311\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"s-nlp/xlmr_formality_classifier\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"s-nlp/xlmr_formality_classifier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tang was employed at private-equity firm Friedman Fleischer & Lowe.', \"San Francisco Mayor Gavin Newsom's withdrawal from the governor's race followed a meeting with top advisers where he was told that, unless he raised $5 million quickly and appeared to be viable, some of the state's biggest unions would throw their support behind Attorney General Jerry Brown.\", 'lol nothing worrying about that.', 'She told Price she wanted to join the Police Explorers, a Boy Scouts group that lets boys and girls learn law enforcement with local police.', 'The prime minister is keen to use the autumn pre-budget statement to announce a new \"fiscal stimulus\", with billions of pounds of extra money for housing, infrastructure projects and training.', \"Those competencies include mastering fundamental legal skills, support of the firm's culture, demonstration of leadership and business skills, and understanding and effectively managing client needs.\", 'His platform contains plans to fund drainage projects.', '\"It\\'s a start.\"', '\"She is not asking for anything over the top, just to be able to find her daughter and let her know she loves her.\"', \"Justice Dinakaran had maintained that he had not acquired an inch of land since his appointment as a judge of HC and even called the collector's report motivated.\", 'Kirk has given few speeches, limited his press releases to a few nuts-and-bolts issues, and shunned almost all requests for interviews.', '\"You never know. Miracles happen,\" she said.', 'Some evidence indicates they have spent at least some of the past 14 years in Mexico, a country that has shown itself as a haven for hiding American kids.', 'Martina Metzler peers at the piles of paper strips spread across four desks in her office.', 'The shares, which traded at more than $61 each in February 2007, have declined 84 percent this year.', 'constently... I work at a Radio Station.']\n"
     ]
    }
   ],
   "source": [
    "id2formality = {0: \"formal\", 1: \"informal\"}\n",
    "\n",
    "# Process texts in smaller batches to reduce memory usage\n",
    "batch_size = 16  # Adjust batch size as needed\n",
    "formality_scores = []\n",
    "\n",
    "for i in range(0, len(df), batch_size):\n",
    "    texts = list(df['sentence'][i:i + batch_size])\n",
    "    print(texts)\n",
    "\n",
    "    # prepare the input\n",
    "    encoding = tokenizer(\n",
    "        texts,\n",
    "        add_special_tokens=True,\n",
    "        return_token_type_ids=True,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "\n",
    "    # inference\n",
    "    output = model(**encoding)\n",
    "\n",
    "    # Collect formality scores for the current batch\n",
    "    batch_scores = [\n",
    "        {id2formality[idx]: score for idx, score in enumerate(text_scores.tolist())}\n",
    "        for text_scores in output.logits.softmax(dim=1)\n",
    "    ]\n",
    "    formality_scores.extend(batch_scores)\n",
    "\n",
    "print(formality_scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
